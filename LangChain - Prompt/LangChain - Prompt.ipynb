{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Prompts?\n",
    "Prompts are the input instructions or quarries given to a model to guide the output\n",
    "\n",
    "---> Static\n",
    "\n",
    "---> Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt.ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a simple ui for a summarizer for given research paper using streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "from langchain_core.prompts import PromptTemplate,load_prompt\n",
    "\n",
    "load_dotenv()\n",
    "model = ChatOpenAI()\n",
    "\n",
    "st.header('Reasearch Tool')\n",
    "\n",
    "paper_input = st.selectbox( \"Select Research Paper Name\", [\"Attention Is All You Need\", \"BERT: Pre-training of Deep Bidirectional Transformers\", \"GPT-3: Language Models are Few-Shot Learners\", \"Diffusion Models Beat GANs on Image Synthesis\"] )\n",
    "\n",
    "style_input = st.selectbox( \"Select Explanation Style\", [\"Beginner-Friendly\", \"Technical\", \"Code-Oriented\", \"Mathematical\"] ) \n",
    "\n",
    "length_input = st.selectbox( \"Select Explanation Length\", [\"Short (1-2 paragraphs)\", \"Medium (3-5 paragraphs)\", \"Long (detailed explanation)\"] )\n",
    "\n",
    "template = load_prompt('template.json')\n",
    "\n",
    "\n",
    "\n",
    "if st.button('Summarize'):\n",
    "    chain = template | model\n",
    "    result = chain.invoke({\n",
    "        'paper_input':paper_input,\n",
    "        'style_input':style_input,\n",
    "        'length_input':length_input\n",
    "    })\n",
    "    st.write(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A promt template is LangChain is a structured way to create prompt dynamically by inserting variable into a predefined template. Instead of hardcoding prompt. PromptTemplate allows you to define paceholders that can be filled in at runtime with different inputs.\n",
    "\n",
    "this makes it reusable, futile, and easy to manage when working with dynamic user inputs or automated workflows.\n",
    "\n",
    "Why use PromptTemplate\n",
    "\n",
    "1. Default validation\n",
    "\n",
    "2. Reusable\n",
    "\n",
    "3. LangChain Ecosystemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# detailed way\n",
    "template2 = PromptTemplate(\n",
    "    template='Greet this person in 5 languages. The name of the person is {name}',\n",
    "    input_variables=['name']\n",
    ")\n",
    "\n",
    "# fill the values of the placeholders\n",
    "prompt = template2.invoke({'name':'nitish'})\n",
    "\n",
    "result = model.invoke(prompt)\n",
    "\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# template\n",
    "template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Please summarize the research paper titled \"{paper_input}\" with the following specifications:\n",
    "Explanation Style: {style_input}  \n",
    "Explanation Length: {length_input}  \n",
    "1. Mathematical Details:  \n",
    "   - Include relevant mathematical equations if present in the paper.  \n",
    "   - Explain the mathematical concepts using simple, intuitive code snippets where applicable.  \n",
    "2. Analogies:  \n",
    "   - Use relatable analogies to simplify complex ideas.  \n",
    "If certain information is not available in the paper, respond with: \"Insufficient information available\" instead of guessing.  \n",
    "Ensure the summary is clear, accurate, and aligned with the provided style and length.\n",
    "\"\"\",\n",
    "input_variables=['paper_input', 'style_input','length_input'],\n",
    "validate_template=True\n",
    ")\n",
    "\n",
    "template.save('template.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## messages.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three types of messages\n",
    "1. System Msg -> Msg given by the us to the assistant \n",
    "2. Human Msg -> Msg given by the user to assistant\n",
    "3. AI Msg -> Msg given by assistant to user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "messages=[\n",
    "    SystemMessage(content='You are a helpful assistant'),\n",
    "    HumanMessage(content='Tell me about LangChain')\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "\n",
    "messages.append(AIMessage(content=result.content))\n",
    "\n",
    "print(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chatbot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retain the history of msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chat_history = [\n",
    "    SystemMessage(content='You are a helpful AI assistant')\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input = input('You: ')\n",
    "    chat_history.append(HumanMessage(content=user_input))\n",
    "    if user_input == 'exit':\n",
    "        break\n",
    "    result = model.invoke(chat_history)\n",
    "    chat_history.append(AIMessage(content=result.content))\n",
    "    print(\"AI: \",result.content)\n",
    "\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke \n",
    "### _1. Single Msg - Single turn stand alone queries\n",
    "#### ___a. Static Msg\n",
    "#### ___b. Dynamic Msg (Prompt Template)\n",
    "### _2. List of msg - multi - turn conversation\n",
    "#### ___a. Static msg (SystemMsg, HumanMsg, AIMsg)\n",
    "#### ___b. Dynamic Msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chat_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate([\n",
    "    ('system', 'You are a helpful {domain} expert'),\n",
    "    ('human', 'Explain in simple terms, what is {topic}')\n",
    "])\n",
    "\n",
    "prompt = chat_template.invoke({'domain':'cricket','topic':'Dusra'})\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## message_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# chat template\n",
    "chat_template = ChatPromptTemplate([\n",
    "    ('system','You are a helpful customer support agent'),\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    ('human','{query}')\n",
    "])\n",
    "\n",
    "chat_history = []\n",
    "# load chat history\n",
    "with open('chat_history.txt') as f:\n",
    "    chat_history.extend(f.readlines())\n",
    "\n",
    "print(chat_history)\n",
    "\n",
    "# create prompt\n",
    "prompt = chat_template.invoke({'chat_history':chat_history, 'query':'Where is my refund'})\n",
    "\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
