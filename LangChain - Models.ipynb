{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Models?\n",
    "\n",
    "The modle component in langChain is acrucial part of the framework, designed to facilitate interaction with various language models and embedding models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### -> Language Models\n",
    "\n",
    "#### ---> LLM\n",
    "\n",
    "#### ---> Chat Models\n",
    "\n",
    "### -> Embedding Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models\n",
    "\n",
    "Language Models are AI systems designed to process, generate and understand natural language text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM_OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct')\n",
    "\n",
    "result = llm.invoke(\"What is the capital of India\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatModel_OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4', temperature=1.5, max_completion_tokens=10)\n",
    "\n",
    "result = model.invoke(\"Write a 5 line poem on cricket\")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatModel_ANTHROPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatAnthropic(model='claude-3-5-sonnet-20241022')\n",
    "\n",
    "result = model.invoke('What is the capital of India')\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatModel_GOOGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model='gemini-1.5-pro')\n",
    "\n",
    "result = model.invoke('What is the capital of India')\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatModel_Hugging_Face_API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "result = model.invoke(\"What is the capital of India\")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatModel_Hugging_Face_Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
    "    task='text-generation',\n",
    "    pipeline_kwargs=dict(\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=100\n",
    "    )\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "result = model.invoke(\"What is the capital of India\")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding_OPENAI_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large', dimensions=32)\n",
    "\n",
    "result = embedding.embed_query(\"Delhi is the capital of India\")\n",
    "\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding_OPENAI_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large', dimensions=32)\n",
    "\n",
    "documents = [\n",
    "    \"Delhi is the capital of India\",\n",
    "    \"Kolkata is the capital of West Bengal\",\n",
    "    \"Paris is the capital of France\"\n",
    "]\n",
    "\n",
    "result = embedding.embed_documents(documents)\n",
    "\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding_Hugging_Face_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "documents = [\n",
    "    \"Delhi is the capital of India\",\n",
    "    \"Kolkata is the capital of West Bengal\",\n",
    "    \"Paris is the capital of France\"\n",
    "]\n",
    "\n",
    "vector = embedding.embed_documents(documents)\n",
    "\n",
    "print(str(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document_Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large', dimensions=300)\n",
    "\n",
    "documents = [\n",
    "    \"Virat Kohli is an Indian cricketer known for his aggressive batting and leadership.\",\n",
    "    \"MS Dhoni is a former Indian captain famous for his calm demeanor and finishing skills.\",\n",
    "    \"Sachin Tendulkar, also known as the 'God of Cricket', holds many batting records.\",\n",
    "    \"Rohit Sharma is known for his elegant batting and record-breaking double centuries.\",\n",
    "    \"Jasprit Bumrah is an Indian fast bowler known for his unorthodox action and yorkers.\"\n",
    "]\n",
    "\n",
    "query = 'tell me about bumrah'\n",
    "\n",
    "doc_embeddings = embedding.embed_documents(documents)\n",
    "query_embedding = embedding.embed_query(query)\n",
    "\n",
    "scores = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "\n",
    "index, score = sorted(list(enumerate(scores)),key=lambda x:x[1])[-1]\n",
    "\n",
    "print(query)\n",
    "print(documents[index])\n",
    "print(\"similarity score is:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirement.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Core\n",
    "langchain\n",
    "langchain-core\n",
    "\n",
    "# OpenAI Integration\n",
    "langchain-openai\n",
    "openai\n",
    "\n",
    "# Anthropic Integration\n",
    "langchain-anthropic\n",
    "\n",
    "# Google Gemini (PaLM) Integration\n",
    "langchain-google-genai\n",
    "google-generativeai\n",
    "\n",
    "# Hugging Face Integration\n",
    "langchain-huggingface\n",
    "transformers\n",
    "huggingface-hub\n",
    "\n",
    "# Environment Variable Management\n",
    "python-dotenv\n",
    "\n",
    "# Machine Learning Utilities\n",
    "numpy\n",
    "scikit-learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
